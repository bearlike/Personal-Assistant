# * Environment file for Meeseeks - Personal Assistant
# * Repository: https://github.com/bearlike/personal-assistant
# - Rename this file to .env to make your application functional and remove unused variables.
# TODO-FUTURE: Convert this file to a YAML format for better readability


# * Meeseeks Settings
# - VERSION: Version of your application (There is no need to change these value)
# - ENVMODE: Environment mode of your application (valid options: dev, prod)
VERSION=1.0.0
ENVMODE=dev
LOG_LEVEL=DEBUG
CACHE_DIR='/path/to/cache/directory'
MASTER_API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'


# * Home Assistant Configuration
# - HA_TOKEN (required): Long-lived access token for Home Assistant
# - HA_URL (required): URL of your Home Assistant server API
# - Can be found at: Click your username in the sidebar > Then click on "Create Token" under "Long-Lived Access Tokens"
# - Refer: https://developers.home-assistant.io/docs/auth_api/#long-lived-access-token
MESEEKS_HOME_ASSISTANT_ENABLED=0
HA_TOKEN=HOME_ASSISTANT_LONG_LIVED_ACCESS_TOKEN
HA_URL=https://homeassistant.server.local/api


# * LLM Gateway (LiteLLM / OpenAI-compatible)
# * Recommended: use a LiteLLM proxy for caching and unified provider access.
# - OPENAI_API_BASE (optional): URL of your OpenAI-compatible server (ex: LiteLLM).
# - OPENAI_BASE_URL (optional): legacy alias used by some clients.
# - OPENAI_API_KEY (required): API key for the OpenAI-compatible endpoint.
# - If OPENAI_API_BASE is set and your model name has no provider prefix,
#   Meeseeks will call openai/<model> automatically.
OPENAI_API_BASE=https://lite-llm.server.local/v1
# OPENAI_BASE_URL=https://lite-llm.server.local/v1
OPENAI_API_KEY=sk-OPENAI_API_KEY


# * Model Selection
# - DEFAULT_MODEL (required): Default model for your application
# - Model names may include provider prefixes (ex: openai/gpt-4o, anthropic/claude-3-5-sonnet)
# - Setting TOOL_MODEL (optional), ACTION_PLAN_MODEL (optional) is optional but can help for load balancing, cost efficiency, or response quality
# - TOOL_MODEL (used by AbstractTool based classes), ACTION_PLAN_MODELS (used by ActionPlanner)
DEFAULT_MODEL=anthropic/claude-3-opus
# TOOL_MODEL=microsoft/phi-3-mini-128k-instruct
ACTION_PLAN_MODEL=openai/gpt-3.5-turbo
#
# * Context Management
# - MEESEEKS_RECENT_EVENT_LIMIT: number of recent transcript events to inject
# - MEESEEKS_CONTEXT_SELECT_THRESHOLD: utilization ratio to trigger context selection
# - MEESEEKS_CONTEXT_SELECTION: set 0 to disable context selection
# - CONTEXT_SELECTOR_MODEL: optional model override for context selection
# - STEP_REFLECTION_MODEL: optional model override for step reflection
# - MEESEEKS_STEP_REFLECTION: set 0 to disable step reflection
MEESEEKS_RECENT_EVENT_LIMIT=8
MEESEEKS_CONTEXT_SELECT_THRESHOLD=0.8
MEESEEKS_CONTEXT_SELECTION=1
# CONTEXT_SELECTOR_MODEL=openai/gpt-3.5-turbo
# STEP_REFLECTION_MODEL=openai/gpt-3.5-turbo


# * MCP Configuration (optional)
# - MESEEKS_MCP_CONFIG: Path to MCP servers JSON config for external tools
# - MESEEKS_TOOL_MANIFEST: (optional) override tool registry manifest (disables auto-discovery)
# - Manifests are auto-generated when only MESEEKS_MCP_CONFIG is set
# - See configs/mcp.example.json + configs/tool-manifest.example.json for templates
MESEEKS_MCP_CONFIG=./configs/mcp.example.json
# MESEEKS_TOOL_MANIFEST=./configs/tool-manifest.example.json


# * Langfuse Configuration for LLM Observability
# - LANGFUSE_HOST (optional): URL of your Langfuse server
# - LANGFUSE_SECRET_KEY (required), LANGFUSE_PUBLIC_KEY (required): Your Langfuse keys
# - Refer: https://langfuse.com/docs/get-started
LANGFUSE_ENABLED=0
LANGFUSE_HOST=https://langfuse.server.local/
LANGFUSE_SECRET_KEY=sk-ex-dummykey-1234-1234-1234-123456789012
LANGFUSE_PUBLIC_KEY=pk-ex-dummykey-1234-1234-1234-123456789012
